{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This notebook explores the results of a project aimed at enhancing the performance of a model for the Kaggle competition focused on STEM-related questions. Initially, a cluster of relevant Wikipedia STEM articles was identified, leading to the compilation of approximately 270K articles. This dataset is available for download [here](https://www.kaggle.com/datasets/mbanaei/stem-wiki-cohere-no-emb).\n",
    "\n",
    "During the data preparation phase, challenges with WikiExtractor were encountered, resulting in missing numbers and paragraphs from the final parsed content. To rectify this, the Wiki API was employed to gather the complete contexts for the same set of articles, with the refined dataset accessible [here](https://www.kaggle.com/datasets/mbanaei/all-paraphs-parsed-expanded). More information regarding this process can be found in the discussion [here](https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/442483).\n",
    "\n",
    "To validate the coverage of the identified articles, this notebook implements a simple retrieval model that utilizes a model trained exclusively on the RACE dataset. The approach focuses on demonstrating how the selected articles not only encompass those present in the training dataset but also cover a majority of the leaderboard (LB) gold articles.\n",
    "\n",
    "Key design choices for this notebook include:\n",
    "- **Context Retrieval**: A basic TF-IDF method is utilized for retrieving contexts from both datasets in response to each question.\n",
    "- **Model Utilization**: The LongFormer Large model is employed for inference, allowing for the processing of longer input contexts without splitting into sentence-level tokens. This choice mitigates out-of-memory (OOM) issues and supports faster inference due to the model's efficient handling of attention mechanisms.\n",
    "- **Fallback Mechanism**: To enhance prediction accuracy, a fallback model based on a public notebook employing an open-book approach is used to make predictions when the primary model demonstrates low confidence in its top choice.\n",
    "\n",
    "While the performance of this model is competitive with other public notebooks, there remain opportunities for improvement in both inference time and overall accuracy, particularly in the context retrieval process, which currently lacks prior indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input  working\n"
     ]
    }
   ],
   "source": [
    "!ls ./kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./kaggle/working/datasets-2.14.4-py3-none-any.whl\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (2023.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (1.23.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (2.28.2)\n",
      "Requirement already satisfied: xxhash in /home/jovyan/.local/lib/python3.10/site-packages (from datasets==2.14.4) (3.3.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (3.8.4)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (0.70.15)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /home/jovyan/.local/lib/python3.10/site-packages (from datasets==2.14.4) (0.16.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (5.4.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (0.3.7)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (4.65.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (2.0.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (23.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (11.0.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (4.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (3.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (1.9.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (22.2.0)\n",
      "Requirement already satisfied: filelock in /home/jovyan/.local/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.4) (3.12.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.4) (4.8.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.4) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.4) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.4) (2022.12.7)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.4) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.4) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.4) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.14.4) (1.16.0)\n",
      "datasets is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n"
     ]
    }
   ],
   "source": [
    "!cp ./kaggle/input/datasets-wheel/datasets-2.14.4-py3-none-any.whl ./kaggle/working\n",
    "!pip install  ./kaggle/working/datasets-2.14.4-py3-none-any.whl\n",
    "!cp ./kaggle/input/backup-806/util_openbook.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "faiss-gpu is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
      "Processing ./kaggle/working/sentence-transformers\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /home/jovyan/.local/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (4.31.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.13.1+cu116)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.14.1+cu116)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.23.5)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.3.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.10.1)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /home/jovyan/.local/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.16.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (5.4.1)\n",
      "Requirement already satisfied: filelock in /home/jovyan/.local/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.12.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.28.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2023.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (23.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/jovyan/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jovyan/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2023.8.8)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/jovyan/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.3.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk->sentence-transformers==2.2.2) (1.2.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk->sentence-transformers==2.2.2) (8.1.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==2.2.2) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence-transformers==2.2.2) (9.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2022.12.7)\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=126125 sha256=dd4136440edd464c5a9baead8fa70699e6fc6867551a149b45733c6cc1728a9a\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/cf/29/94/952edff7a57baedcc598dd3582cf671d803cd3205aa09632b4\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: sentence-transformers\n",
      "  Attempting uninstall: sentence-transformers\n",
      "    Found existing installation: sentence-transformers 2.2.2\n",
      "    Uninstalling sentence-transformers-2.2.2:\n",
      "      Successfully uninstalled sentence-transformers-2.2.2\n",
      "Successfully installed sentence-transformers-2.2.2\n",
      "Processing ./kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl\n",
      "blingfire is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
      "Processing ./kaggle/input/llm-whls/transformers-4.31.0-py3-none-any.whl\n",
      "transformers is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
      "Processing ./kaggle/input/llm-whls/peft-0.4.0-py3-none-any.whl\n",
      "peft is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
      "Processing ./kaggle/input/llm-whls/trl-0.5.0-py3-none-any.whl\n",
      "trl is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n"
     ]
    }
   ],
   "source": [
    "# installing offline dependencies\n",
    "!pip install -U ./kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!cp -rf ./kaggle/input/sentence-transformers-222/sentence-transformers ./kaggle/working/sentence-transformers\n",
    "!pip install -U ./kaggle/working/sentence-transformers\n",
    "!pip install -U ./kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl\n",
    "\n",
    "!pip install --user --no-index --no-deps ./kaggle/input/llm-whls/transformers-4.31.0-py3-none-any.whl\n",
    "!pip install --user --no-index --no-deps ./kaggle/input/llm-whls/peft-0.4.0-py3-none-any.whl\n",
    "!pip install --user --no-index --no-deps ./kaggle/input/llm-whls/trl-0.5.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/kaggle_competition_v2/kaggle_v2_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Batches: 100%|██████████| 13/13 [00:00<00:00, 13.08it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 3285.57it/s]\n",
      "100%|██████████| 28/28 [01:33<00:00,  3.33s/it]\n",
      "100%|██████████| 3546/3546 [00:00<00:00, 1120883.41it/s]\n",
      "100%|██████████| 3546/3546 [00:07<00:00, 486.14it/s]\n",
      "Batches: 100%|██████████| 10459/10459 [00:47<00:00, 222.50it/s]\n",
      "Batches: 100%|██████████| 13/13 [00:00<00:00, 127.60it/s]\n",
      "100%|██████████| 200/200 [00:08<00:00, 23.50it/s]\n",
      "Map:   0%|          | 0/200 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 482.12 examples/s]\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from util_openbook import get_contexts, generate_openbook_output\n",
    "import pickle\n",
    "\n",
    "get_contexts()\n",
    "generate_openbook_output()\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "backup_model_predictions = pd.read_csv(\"submission_backup.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from datasets import load_dataset, load_from_disk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "from transformers import LongformerTokenizer, LongformerForMultipleChoice\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!cp -r ./kaggle/input/stem-wiki-cohere-no-emb ./kaggle/working\n",
    "!cp -r ./kaggle/input/all-paraphs-parsed-expanded ./kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitList(mylist, chunk_size):\n",
    "    return [mylist[offs:offs+chunk_size] for offs in range(0, len(mylist), chunk_size)]\n",
    "\n",
    "def get_relevant_documents_parsed(df_valid):\n",
    "    df_chunk_size=600\n",
    "    paraphs_parsed_dataset = load_from_disk(\"./kaggle/working/all-paraphs-parsed-expanded\")\n",
    "    modified_texts = paraphs_parsed_dataset.map(lambda example:\n",
    "                                             {'temp_text':\n",
    "                                              f\"{example['title']} {example['section']} {example['text']}\".replace('\\n',\" \").replace(\"'\",\"\")},\n",
    "                                             num_proc=2)[\"temp_text\"]\n",
    "    \n",
    "    all_articles_indices = []\n",
    "    all_articles_values = []\n",
    "    for idx in tqdm(range(0, df_valid.shape[0], df_chunk_size)):\n",
    "        df_valid_ = df_valid.iloc[idx: idx+df_chunk_size]\n",
    "    \n",
    "        articles_indices, merged_top_scores = retrieval(df_valid_, modified_texts)\n",
    "        all_articles_indices.append(articles_indices)\n",
    "        all_articles_values.append(merged_top_scores)\n",
    "        \n",
    "    article_indices_array =  np.concatenate(all_articles_indices, axis=0)\n",
    "    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n",
    "    \n",
    "    top_per_query = article_indices_array.shape[1]\n",
    "    articles_flatten = [(\n",
    "                         articles_values_array[index],\n",
    "                         paraphs_parsed_dataset[idx.item()][\"title\"],\n",
    "                         paraphs_parsed_dataset[idx.item()][\"text\"],\n",
    "                        )\n",
    "                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n",
    "    retrieved_articles = SplitList(articles_flatten, top_per_query)\n",
    "    return retrieved_articles\n",
    "\n",
    "\n",
    "\n",
    "def get_relevant_documents(df_valid):\n",
    "    df_chunk_size=800\n",
    "    \n",
    "    cohere_dataset_filtered = load_from_disk(\"./kaggle/working/stem-wiki-cohere-no-emb\")\n",
    "    modified_texts = cohere_dataset_filtered.map(lambda example:\n",
    "                                             {'temp_text':\n",
    "                                              unicodedata.normalize(\"NFKD\", f\"{example['title']} {example['text']}\").replace('\"',\"\")},\n",
    "                                             num_proc=2)[\"temp_text\"]\n",
    "    \n",
    "    all_articles_indices = []\n",
    "    all_articles_values = []\n",
    "    for idx in tqdm(range(0, df_valid.shape[0], df_chunk_size)):\n",
    "        df_valid_ = df_valid.iloc[idx: idx+df_chunk_size]\n",
    "    \n",
    "        articles_indices, merged_top_scores = retrieval(df_valid_, modified_texts)\n",
    "        all_articles_indices.append(articles_indices)\n",
    "        all_articles_values.append(merged_top_scores)\n",
    "        \n",
    "    article_indices_array =  np.concatenate(all_articles_indices, axis=0)\n",
    "    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n",
    "    \n",
    "    top_per_query = article_indices_array.shape[1]\n",
    "    articles_flatten = [(\n",
    "                         articles_values_array[index],\n",
    "                         cohere_dataset_filtered[idx.item()][\"title\"],\n",
    "                         unicodedata.normalize(\"NFKD\", cohere_dataset_filtered[idx.item()][\"text\"]),\n",
    "                        )\n",
    "                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n",
    "    retrieved_articles = SplitList(articles_flatten, top_per_query)\n",
    "    return retrieved_articles\n",
    "\n",
    "\n",
    "\n",
    "def retrieval(df_valid, modified_texts):\n",
    "    \n",
    "    corpus_df_valid = df_valid.apply(lambda row:\n",
    "                                     f'{row[\"prompt\"]}\\n{row[\"prompt\"]}\\n{row[\"prompt\"]}\\n{row[\"A\"]}\\n{row[\"B\"]}\\n{row[\"C\"]}\\n{row[\"D\"]}\\n{row[\"E\"]}',\n",
    "                                     axis=1).values\n",
    "    vectorizer1 = TfidfVectorizer(ngram_range=(1,2),\n",
    "                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\\"|\\'\",\n",
    "                                 stop_words=stop_words)\n",
    "    vectorizer1.fit(corpus_df_valid)\n",
    "    vocab_df_valid = vectorizer1.get_feature_names_out()\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,2),\n",
    "                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\\"|\\'\",\n",
    "                                 stop_words=stop_words,\n",
    "                                 vocabulary=vocab_df_valid)\n",
    "    vectorizer.fit(modified_texts[:500000])\n",
    "    corpus_tf_idf = vectorizer.transform(corpus_df_valid)\n",
    "    \n",
    "    print(f\"length of vectorizer vocab is {len(vectorizer.get_feature_names_out())}\")\n",
    "\n",
    "    chunk_size = 100000\n",
    "    top_per_chunk = 10\n",
    "    top_per_query = 10\n",
    "\n",
    "    all_chunk_top_indices = []\n",
    "    all_chunk_top_values = []\n",
    "\n",
    "    for idx in tqdm(range(0, len(modified_texts), chunk_size)):\n",
    "        wiki_vectors = vectorizer.transform(modified_texts[idx: idx+chunk_size])\n",
    "        # [total_vocab:total_docs]*[total_vocab:chunk_size].T\n",
    "        temp_scores = (corpus_tf_idf * wiki_vectors.T).toarray()\n",
    "        chunk_top_indices = temp_scores.argpartition(-top_per_chunk, axis=1)[:, -top_per_chunk:]\n",
    "        chunk_top_values = temp_scores[np.arange(temp_scores.shape[0])[:, np.newaxis], chunk_top_indices]\n",
    "\n",
    "        all_chunk_top_indices.append(chunk_top_indices + idx)\n",
    "        all_chunk_top_values.append(chunk_top_values)\n",
    "\n",
    "    top_indices_array = np.concatenate(all_chunk_top_indices, axis=1)\n",
    "    top_values_array = np.concatenate(all_chunk_top_values, axis=1)\n",
    "    \n",
    "    merged_top_scores = np.sort(top_values_array, axis=1)[:,-top_per_query:]\n",
    "    merged_top_indices = top_values_array.argsort(axis=1)[:,-top_per_query:]\n",
    "    articles_indices = top_indices_array[np.arange(top_indices_array.shape[0])[:, np.newaxis], merged_top_indices]\n",
    "    \n",
    "    return articles_indices, merged_top_scores\n",
    "\n",
    "\n",
    "def prepare_answering_input(\n",
    "        tokenizer, \n",
    "        question,  \n",
    "        options,   \n",
    "        context,   \n",
    "        max_seq_length=4096,\n",
    "    ):\n",
    "    c_plus_q   = context + ' ' + tokenizer.bos_token + ' ' + question\n",
    "    c_plus_q_4 = [c_plus_q] * len(options)\n",
    "    tokenized_examples = tokenizer(\n",
    "        c_plus_q_4, options,\n",
    "        max_length=max_seq_length,\n",
    "        padding=\"longest\",\n",
    "        truncation=False,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = tokenized_examples['input_ids'].unsqueeze(0)\n",
    "    attention_mask = tokenized_examples['attention_mask'].unsqueeze(0)\n",
    "    example_encoded = {\n",
    "        \"input_ids\": input_ids.to(model.device.index),\n",
    "        \"attention_mask\": attention_mask.to(model.device.index),\n",
    "    }\n",
    "    return example_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['each', 'you', 'the', 'use', 'used',\n",
    "                  'where', 'themselves', 'nor', \"it's\", 'how', \"don't\", 'just', 'your',\n",
    "                  'about', 'himself', 'with', \"weren't\", 'hers', \"wouldn't\", 'more', 'its', 'were',\n",
    "                  'his', 'their', 'then', 'been', 'myself', 're', 'not',\n",
    "                  'ours', 'will', 'needn', 'which', 'here', 'hadn', 'it', 'our', 'there', 'than',\n",
    "                  'most', \"couldn't\", 'both', 'some', 'for', 'up', 'couldn', \"that'll\",\n",
    "                  \"she's\", 'over', 'this', 'now', 'until', 'these', 'few', 'haven',\n",
    "                  'of', 'wouldn', 'into', 'too', 'to', 'very', 'shan', 'before', 'the', 'they',\n",
    "                  'between', \"doesn't\", 'are', 'was', 'out', 'we', 'me',\n",
    "                  'after', 'has', \"isn't\", 'have', 'such', 'should', 'yourselves', 'or', 'during', 'herself',\n",
    "                  'doing', 'in', \"shouldn't\", \"won't\", 'when', 'do', 'through', 'she',\n",
    "                  'having', 'him', \"haven't\", 'against', 'itself', 'that',\n",
    "                  'did', 'theirs', 'can', 'those',\n",
    "                  'own', 'so', 'and', 'who', \"you've\", 'yourself', 'her', 'he', 'only',\n",
    "                  'what', 'ourselves', 'again', 'had', \"you'd\", 'is', 'other',\n",
    "                  'why', 'while', 'from', 'them', 'if', 'above', 'does', 'whom',\n",
    "                  'yours', 'but', 'being', \"wasn't\", 'be']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = pd.read_csv(\"./kaggle/input/kaggle-llm-science-exam/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=2): 100%|██████████| 2101279/2101279 [00:42<00:00, 49008.47 examples/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]/home/jovyan/kaggle_competition_v2/kaggle_v2_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'd', 'doesn', 'don', 'isn', 'll', 's', 'shouldn', 't', 've', 'wasn', 'weren', 'won'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of vectorizer vocab is 11222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▍         | 1/22 [00:08<02:59,  8.53s/it]\u001b[A\n",
      "  9%|▉         | 2/22 [00:16<02:47,  8.38s/it]\u001b[A\n",
      " 14%|█▎        | 3/22 [00:25<02:41,  8.48s/it]\u001b[A\n",
      " 18%|█▊        | 4/22 [00:33<02:31,  8.44s/it]\u001b[A\n",
      " 23%|██▎       | 5/22 [00:42<02:22,  8.37s/it]\u001b[A\n",
      " 27%|██▋       | 6/22 [00:50<02:14,  8.39s/it]\u001b[A\n",
      " 32%|███▏      | 7/22 [00:59<02:07,  8.47s/it]\u001b[A\n",
      " 36%|███▋      | 8/22 [01:07<01:57,  8.42s/it]\u001b[A\n",
      " 41%|████      | 9/22 [01:15<01:49,  8.45s/it]\u001b[A\n",
      " 45%|████▌     | 10/22 [01:24<01:41,  8.49s/it]\u001b[A\n",
      " 50%|█████     | 11/22 [01:33<01:33,  8.51s/it]\u001b[A\n",
      " 55%|█████▍    | 12/22 [01:41<01:25,  8.52s/it]\u001b[A\n",
      " 59%|█████▉    | 13/22 [01:50<01:16,  8.51s/it]\u001b[A\n",
      " 64%|██████▎   | 14/22 [01:58<01:07,  8.49s/it]\u001b[A\n",
      " 68%|██████▊   | 15/22 [02:07<00:59,  8.51s/it]\u001b[A\n",
      " 73%|███████▎  | 16/22 [02:15<00:51,  8.51s/it]\u001b[A\n",
      " 77%|███████▋  | 17/22 [02:24<00:42,  8.53s/it]\u001b[A\n",
      " 82%|████████▏ | 18/22 [02:32<00:34,  8.53s/it]\u001b[A\n",
      " 86%|████████▋ | 19/22 [02:41<00:25,  8.54s/it]\u001b[A\n",
      " 91%|█████████ | 20/22 [02:49<00:17,  8.50s/it]\u001b[A\n",
      " 95%|█████████▌| 21/22 [02:58<00:08,  8.47s/it]\u001b[A\n",
      "100%|██████████| 22/22 [02:58<00:00,  8.10s/it]\u001b[A\n",
      "100%|██████████| 1/1 [03:38<00:00, 218.53s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "289"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_articles_parsed = get_relevant_documents_parsed(df_valid)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=2): 100%|██████████| 2781652/2781652 [01:47<00:00, 25791.61 examples/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of vectorizer vocab is 11222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▎         | 1/28 [00:06<02:48,  6.23s/it]\u001b[A\n",
      "  7%|▋         | 2/28 [00:12<02:36,  6.03s/it]\u001b[A\n",
      " 11%|█         | 3/28 [00:18<02:29,  5.96s/it]\u001b[A\n",
      " 14%|█▍        | 4/28 [00:23<02:22,  5.94s/it]\u001b[A\n",
      " 18%|█▊        | 5/28 [00:29<02:14,  5.84s/it]\u001b[A\n",
      " 21%|██▏       | 6/28 [00:35<02:08,  5.85s/it]\u001b[A\n",
      " 25%|██▌       | 7/28 [00:41<02:01,  5.79s/it]\u001b[A\n",
      " 29%|██▊       | 8/28 [00:46<01:54,  5.74s/it]\u001b[A\n",
      " 32%|███▏      | 9/28 [00:52<01:48,  5.71s/it]\u001b[A\n",
      " 36%|███▌      | 10/28 [00:57<01:41,  5.65s/it]\u001b[A\n",
      " 39%|███▉      | 11/28 [01:03<01:36,  5.69s/it]\u001b[A\n",
      " 43%|████▎     | 12/28 [01:09<01:29,  5.60s/it]\u001b[A\n",
      " 46%|████▋     | 13/28 [01:14<01:22,  5.51s/it]\u001b[A\n",
      " 50%|█████     | 14/28 [01:19<01:16,  5.50s/it]\u001b[A\n",
      " 54%|█████▎    | 15/28 [01:25<01:11,  5.49s/it]\u001b[A\n",
      " 57%|█████▋    | 16/28 [01:30<01:05,  5.44s/it]\u001b[A\n",
      " 61%|██████    | 17/28 [01:36<01:00,  5.47s/it]\u001b[A\n",
      " 64%|██████▍   | 18/28 [01:41<00:54,  5.42s/it]\u001b[A\n",
      " 68%|██████▊   | 19/28 [01:46<00:48,  5.38s/it]\u001b[A\n",
      " 71%|███████▏  | 20/28 [01:52<00:42,  5.35s/it]\u001b[A\n",
      " 75%|███████▌  | 21/28 [01:57<00:37,  5.33s/it]\u001b[A\n",
      " 79%|███████▊  | 22/28 [02:02<00:31,  5.31s/it]\u001b[A\n",
      " 82%|████████▏ | 23/28 [02:07<00:26,  5.28s/it]\u001b[A\n",
      " 86%|████████▌ | 24/28 [02:12<00:20,  5.22s/it]\u001b[A\n",
      " 89%|████████▉ | 25/28 [02:17<00:15,  5.15s/it]\u001b[A\n",
      " 93%|█████████▎| 26/28 [02:22<00:10,  5.09s/it]\u001b[A\n",
      " 96%|█████████▋| 27/28 [02:27<00:04,  4.97s/it]\u001b[A\n",
      "100%|██████████| 28/28 [02:31<00:00,  5.40s/it]\u001b[A\n",
      "100%|██████████| 1/1 [02:59<00:00, 179.02s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_articles = get_relevant_documents(df_valid)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LongformerTokenizer.from_pretrained(\"./kaggle/input/longformer-race-model/longformer_qa_model\")\n",
    "model = LongformerForMultipleChoice.from_pretrained(\"./kaggle/input/longformer-race-model/longformer_qa_model\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [03:06<00:00,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "submit_ids = []\n",
    "\n",
    "for index in tqdm(range(df_valid.shape[0])):\n",
    "    columns = df_valid.iloc[index].values\n",
    "    submit_ids.append(columns[0])\n",
    "    question = columns[1]\n",
    "    options = [columns[2], columns[3], columns[4], columns[5], columns[6]]\n",
    "    context1 = f\"{retrieved_articles[index][-4][2]}\\n{retrieved_articles[index][-3][2]}\\n{retrieved_articles[index][-2][2]}\\n{retrieved_articles[index][-1][2]}\"\n",
    "    context2 = f\"{retrieved_articles_parsed[index][-3][2]}\\n{retrieved_articles_parsed[index][-2][2]}\\n{retrieved_articles_parsed[index][-1][2]}\"\n",
    "    inputs1 = prepare_answering_input(\n",
    "        tokenizer=tokenizer, question=question,\n",
    "        options=options, context=context1,\n",
    "        )\n",
    "    inputs2 = prepare_answering_input(\n",
    "        tokenizer=tokenizer, question=question,\n",
    "        options=options, context=context2,\n",
    "        )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs1 = model(**inputs1)    \n",
    "        losses1 = -outputs1.logits[0].detach().cpu().numpy()\n",
    "        probability1 = torch.softmax(torch.tensor(-losses1), dim=-1)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        outputs2 = model(**inputs2)\n",
    "        losses2 = -outputs2.logits[0].detach().cpu().numpy()\n",
    "        probability2 = torch.softmax(torch.tensor(-losses2), dim=-1)\n",
    "        \n",
    "    probability_ = (probability1 + probability2)/2\n",
    "\n",
    "    if probability_.max() > 0.4:\n",
    "        predict = np.array(list(\"ABCDE\"))[np.argsort(probability_)][-3:].tolist()[::-1]\n",
    "    else:\n",
    "        predict = backup_model_predictions.iloc[index].prediction.replace(\" \",\"\")\n",
    "    predictions.append(predict)\n",
    "\n",
    "predictions = [\" \".join(i) for i in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'id':submit_ids,'prediction':predictions}).to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9658333333333334\n"
     ]
    }
   ],
   "source": [
    "from functions import mapk\n",
    "df = pd.read_csv('submission.csv')\n",
    "answer_df = pd.read_csv('datasets/train.csv')\n",
    "answer = answer_df['answer'].tolist()\n",
    "df['prediction'] = df['prediction'].str.split()\n",
    "prediction= df['prediction'].tolist()\n",
    "res = mapk(answer, prediction, 3)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (kaggle_v2)",
   "language": "python",
   "name": "kaggle_v2_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
