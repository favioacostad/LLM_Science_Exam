# Project: Evaluating Language Model Performance in Science Question-Answering under Resource Constraints

This repository documents my work on the Kaggle competition **[Kaggle - LLM Science Exam](https://www.kaggle.com/competitions/kaggle-llm-science-exam)**. The competition challenges participants to create models capable of answering complex, science-based questions generated by a Large Language Model (LLM). My goal is to develop high-performing LLMs that can handle intricate scientific questions, while also operating effectively within resource-constrained environments.

## Project Overview
The objective of this project is to develop efficient LLMs capable of answering complex science questions while optimizing performance under resource constraints. For this, I utilized **Llama LLM models** of various sizes (3B, 7B, and 11B). However, due to their large size, these models are **not included in this repository**, so the `finetuned_models` folder remains empty.

## Contextual Analysis with Retrieval-Augmented Generation (RAG)
I integrated **Retrieval-Augmented Generation (RAG)** into the workflow to enhance contextual analysis. RAG helps retrieve relevant information from external sources, which improves the modelâ€™s ability to generate accurate and contextually appropriate answers to science-based questions.

## Fine-Tuning with QLoRA
To fine-tune the models for specific text-generation tasks efficiently, I used **[QLoRA (Efficient Finetuning of Quantized LLMs)](https://arxiv.org/abs/2305.14314)**. QLoRA is a cutting-edge method that quantizes pretrained models to 4-bit precision and incorporates Low-Rank Adapters, enabling fine-tuning even on a single GPU. This technique, combined with the **[PEFT library](https://huggingface.co/docs/peft/)**, provided an efficient way to optimize the models for performance in the competition.

## Repository Structure
The repository is organized as follows:

- **additional_datasets/**: Contains supplementary datasets used to enhance or validate model performance.
- **context/**: Stores preprocessed context data files used with RAG for improved contextual analysis.
- **datasets/**: Holds the primary datasets provided for the Kaggle competition, including both training and test data.
- **finetuned_models/**: This folder is currently empty because the large Llama models are not included in the repository due to their size.
- **kaggle/**: Contains utility functions, including those used to calculate the Mean Average Precision @ 3 (MAP@3) evaluation metric.
- **modelling/**: Contains notebooks that implement the general model approach without RAG for direct text generation.
- **modelling_with_rag/**: Contains notebooks that implement the RAG-based approach for context-enhanced text generation.
- **outputs/**: Stores the answers generated by the models for the test dataset. These files are necessary for calculating performance metrics such as accuracy and MAP@3.
- **results/**: Contains a CSV file summarizing the results of all tested model configurations, including accuracy and MAP@3 scores.

## Notes
- **Evaluation**: Submissions to the Kaggle competition are evaluated based on the Mean Average Precision @ 3 (MAP@3) metric.
- **Requirements**: Ensure you have installed the necessary libraries such as `transformers`, `peft`, and other dependencies specified in `requirements.txt`.