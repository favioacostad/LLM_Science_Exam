# Project: Evaluating Language Model Performance in Science Question-Answering under Resource Constraints

This repository documents our work on the Kaggle competition **[Kaggle - LLM Science Exam](https://www.kaggle.com/competitions/kaggle-llm-science-exam)**. This competition challenges participants to create models capable of answering complex, science-based questions generated by a Large Language Model (LLM). 

## Project Overview
Our objective is to develop high-performing LLMs that can handle intricate scientific questions, while also operating effectively within resource-constrained environments. For this project, we used **Llama LLM models** with varying parameter sizes, including 3B, 7B, and even 11B models. Unfortunately, due to their substantial size, these models are **not included in the repository**—thus, the `finetuned_models` folder is empty.

## Contextual Analysis with Retrieval-Augmented Generation (RAG)
We incorporated **Retrieval-Augmented Generation (RAG)** to enhance contextual analysis. This approach aids in retrieving relevant information, which helps improve the model’s ability to answer science-based questions more accurately.

## Fine-Tuning with QLoRA
To adapt the LLMs for specific text-generation tasks and fine-tune them efficiently, we applied **[QLoRA (Efficient Finetuning of Quantized LLMs)](https://arxiv.org/abs/2305.14314)**. QLoRA is a highly efficient fine-tuning method that quantizes pretrained LLMs to just 4 bits and incorporates Low-Rank Adapters, allowing us to fine-tune these models on a single GPU! We implemented QLoRA with the support of the **[PEFT library](https://huggingface.co/docs/peft/)**.

## Repository Structure
- **additional_datasets/**: Contains any supplementary datasets used to enhance or validate model performance.
- **context/**: Stores preprocessed context data files used with Retrieval-Augmented Generation (RAG) for improved contextual analysis.
- **datasets/**: Holds the main datasets provided for the Kaggle competition, including training and test data.
- **finetuned_models/**: This folder is empty as large Llama models were not included due to their size.
- **kaggle/**: Contains utility functions, including those used to calculate the Mean Average Precision @ 3 (MAP@3) evaluation metric.
- **modelling/**: Contains notebooks implementing our general model approach without RAG for direct text generation.
- **modelling_with_rag/**: Contains notebooks implementing the approach with RAG for context-enhanced text generation.
- **outputs/**: Stores the answers generated by each model for the test dataset. These files are necessary for calculating performance metrics such as accuracy and MAP@3.
- **results/**: Contains a CSV file summarizing the overall results of all model configurations tested, including accuracy and MAP@3 scores.

## Notes
- **Evaluation**: Submissions to the Kaggle competition are evaluated based on the Mean Average Precision @ 3 (MAP@3) metric.
- **Requirements**: Ensure you have installed necessary libraries like `transformers`, `peft`, and other dependencies listed in `requirements.txt`.