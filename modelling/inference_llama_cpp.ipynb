{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Fine-Tuning with LLaMA.cpp\n",
    "\n",
    "This notebook provides a framework for fine-tuning LLaMA models using **LLaMA.cpp**, a lightweight and efficient implementation of Meta's LLaMA models. LLaMA.cpp is designed to run LLaMA models on modest hardware by leveraging optimizations that reduce memory usage and enhance performance. This makes it an excellent option for scenarios with limited computational resources.\n",
    "\n",
    "LLaMA models, including LLaMA 2, offer parameter sizes ranging from 7B to 70B. However, due to memory and processing requirements, larger models may need more powerful hardware. By using LLaMA.cpp, we aim to make it feasible to fine-tune and deploy these models effectively even on devices with limited resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6VTFCw9EALA",
    "outputId": "e574a65d-dacc-4776-acb9-8d2acbad426f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.agents import load_tools, initialize_agent\n",
    "import langchain\n",
    "langchain.debug = False\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3bhkibcfqJDp",
    "outputId": "1c44d6ce-c82d-4ccb-f1ef-674512ea1211"
   },
   "source": [
    "dir_path = '/content/drive/My\\ Drive/Asesoftware/Competencia\\ Kaggle/'\n",
    "%cd $dir_path\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GA_J4-_DIAXc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "# Verbose is required to pass to the callback manager\n",
    "# callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instanciating llama model with parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B8hzl0oOoVwF",
    "outputId": "00974b82-e1f1-40dc-da8b-506a822c59ab",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LLama pipeline model\n",
    "llm = LlamaCpp(\n",
    "    model_path = \"files/llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
    "    temperature = 0.1,\n",
    "    max_tokens = 2000,\n",
    "    top_p = 1,\n",
    "    # callback_manager = callback_manager,\n",
    "    verbose = True,\n",
    "    n_gpu_layers=35,\n",
    "    n_batch = 512,\n",
    "    n_ctx=4096\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the agent, and the conversation window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z76KJJYcVAmU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Memory buffer set for 2 messages\n",
    "memory = ConversationBufferWindowMemory(memory_key = 'chat_history', k = 2, return_messages = True, output_key = \"output\")\n",
    "\n",
    "# Agent configuration\n",
    "tools = load_tools(['llm-math'], llm)\n",
    "\n",
    "# Agent initialisation\n",
    "agent = initialize_agent(\n",
    "    agent = \"chat-conversational-react-description\",\n",
    "    tools = tools,\n",
    "    llm = llm,\n",
    "    verbose = True,\n",
    "    early_stopping_method = 'generate',\n",
    "    memory = memory,\n",
    "    handle_parsing_errors = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading train data from csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And Sorting it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "kJWQoybTR-3-",
    "outputId": "50acaaae-c4f0-4e13-d714-a74035f5f66e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train.csv')\n",
    "train_data['answer_contents'] = train_data.apply(lambda row: row[row['answer']], axis = 1)\n",
    "test_data = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-yB6kZPxiyWT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_idx = train_data['prompt'].str.len().sort_values().index\n",
    "train_data_sorted = train_data.reindex(new_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing tags for prompt engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iUvIZrSEWJJ_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure system message tags\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function and variables to format the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90lHj3-TX4Dl",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def human_prompt(n: int = 0):\n",
    "    message = train_data.iloc[n]\n",
    "\n",
    "    q = message['prompt']\n",
    "    A = message['A']\n",
    "    B = message['B']\n",
    "    C = message['C']\n",
    "    D = message['D']\n",
    "    E = message['E']\n",
    "\n",
    "    # instruction = B_INST + \"Pick the most accurate letter of the next multi choice question:\" + E_INST\n",
    "\n",
    "    question = \"\"\"\n",
    "    \\nUser: {question}\n",
    "\n",
    "    A. {answer_1}\n",
    "    B. {answer_2}\n",
    "    C. {answer_3}\n",
    "    D. {answer_4}\n",
    "    E. {answer_5}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(template = question, input_variables = ['question', 'answer_1', 'answer_2', 'answer_3', 'answer_4', 'answer_5'])\n",
    "    # final_prompt = instruction + prompt.format(answer_1 = A, answer_5 = E, answer_4 = D, question = q, answer_3 = C, answer_2= B)\n",
    "    final_prompt = prompt.format(answer_1 = A, answer_5 = E, answer_4 = D, question = q, answer_3 = C, answer_2= B)\n",
    "\n",
    "    return final_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_prompt = \"<s>\" + B_SYS + \"\"\"Assistant will answer a multi choice question by giving 3 and only 3 letters from the options given. The letters will be separated by comma. The order of the answers given by assistant are from the most likely correct to the less likely.\n",
    "No explanation needed for the answers. Assistant never ask for anything. Assistant never ask for answers.\n",
    "\n",
    "Here is a previous conversation between the Assistant and the User:\n",
    "\n",
    "\\nUser: What is the chemical formula of water\n",
    "\n",
    "A. H2O\n",
    "B. O2\n",
    "C. NACL\n",
    "D. C2H5OH\n",
    "E. O3\n",
    "\n",
    "Assistant: (A, B, E).\n",
    "\n",
    "\n",
    "\\nUser: What type of organism is commonly used in preparation of foods such as cheese and yogurt\n",
    "\n",
    "A. viruses\n",
    "B. protozoa\n",
    "C. cells\n",
    "D. gymnosperms\n",
    "E. mesophilic organisms\n",
    "\n",
    "Assistant: (E, C, B).\n",
    "\n",
    "\n",
    "\\nUser: What is the least dangerous radioactive decay\n",
    "\n",
    "A. zeta decay\n",
    "B. beta decay\n",
    "C. gamma decay\n",
    "D. alpha decay\n",
    "E. all of the above\n",
    "\n",
    "Assistant: (D, C, B).\n",
    "\n",
    "\n",
    "\\nUser: What phenomenon makes global winds blow northeast to southwest or the reverse in the northern hemisphere and northwest to southeast or the reverse in the southern hemisphere?\n",
    "\n",
    "A. hurricanes\n",
    "B. tropical effect\n",
    "C. muon effect\n",
    "D. centrifugal effect\n",
    "E. coriolis effect\n",
    "\n",
    "Assistant: (E, C, A).\n",
    "\n",
    "\n",
    "\\nUser: Kilauea in hawaii is the world\\u2019s most continuously active volcano. very active volcanoes characteristically eject red-hot rocks and lava rather than this?\n",
    "\n",
    "A. carbon and smog\n",
    "B. smoke and ash\n",
    "C. greenhouse gases\n",
    "D. magma\n",
    "E. fire\n",
    "\n",
    "Assistant: (B, E, A).\"\"\" + E_SYS\n",
    "new_prompt = agent.agent.create_prompt(system_message = system_prompt, tools = tools)\n",
    "agent.agent.llm_chain.prompt = new_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ans = []\n",
    "model_ans = []\n",
    "for i in tqdm(range(train_data.shape[0])):\n",
    "    t = []\n",
    "    tmp = train_data.iloc[i]\n",
    "    t.append(tmp['id'])\n",
    "    t.append(tmp['answer'])\n",
    "    try:\n",
    "        res = llm(system_prompt + human_prompt(i))\n",
    "        model_ans.append((tmp['id'], res))\n",
    "        l = res.split(':')[1].split('.')[0].strip().replace('(', '').replace(')', '').strip().split(', ')\n",
    "        if len(l[0]) == 1:\n",
    "            t.extend(l)\n",
    "        else:\n",
    "            #if the answer is empty, append an empty response. \n",
    "            t.append('-')\n",
    "            t.append('-')\n",
    "            t.append('-')\n",
    "    except:\n",
    "        t.append('-')\n",
    "        t.append('-')\n",
    "        t.append('-')\n",
    "    ans.append(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting and saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ans = pd.DataFrame(ans, columns=['id', 'answer', 'prediction1', 'prediction2', 'prediction3'])\n",
    "ans.fillna('-', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['prediction1', 'prediction2', 'prediction3']\n",
    "ans['prediction'] = ans[cols].apply(lambda x: ' '.join(x.values.astype(str)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols_to_delete = ['answer', 'prediction1', 'prediction2', 'prediction3']\n",
    "ans.drop(cols_to_delete, axis=1, inplace=True)\n",
    "ans.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
